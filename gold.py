# -*- coding: utf-8 -*-
# é»„é‡‘ä»·æ ¼é¢„æµ‹å®Œæ•´ç¤ºä¾‹ï¼ˆ2025å¹´3æœˆæ›´æ–°ç‰ˆï¼‰
import os
import numpy as np
import pandas as pd
import akshare as ak
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import logging
import datetime
import matplotlib.font_manager as fm
import tensorflow as tf
from sklearn.model_selection import KFold, TimeSeriesSplit
from typing import Optional, Dict, Any, Tuple
import json
import time
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# é…ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
try:
    # å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']  # ä¼˜å…ˆä½¿ç”¨çš„å­—ä½“åˆ—è¡¨
    plt.rcParams['axes.unicode_minus'] = False  # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
    # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æœ‰å¯ç”¨çš„ä¸­æ–‡å­—ä½“
    fonts = [f.name for f in fm.fontManager.ttflist]
    logger.info(f"å¯ç”¨å­—ä½“: {[f for f in fonts if 'é»‘' in f or 'Hei' in f or 'sans' in f.lower()][:5]}")
except Exception as e:
    logger.warning(f"è®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥: {e}ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤º")

# æ•°æ®èŽ·å–æ¨¡å—
def fetch_gold_data(symbol: str = "AU0", 
                   start_date: Optional[str] = None, 
                   end_date: Optional[str] = None) -> pd.DataFrame:
    """èŽ·å–é»„é‡‘æ•°æ®"""
    try:
        logger.info(f"æ­£åœ¨èŽ·å–{symbol}åˆçº¦æ•°æ®...")
        df = ak.futures_zh_daily_sina(symbol=symbol)
        df['date'] = pd.to_datetime(df['date'])
        
        # æ•°æ®ç­›é€‰
        if start_date:
            df = df[df['date'] >= pd.to_datetime(start_date)]
        if end_date:
            df = df[df['date'] <= pd.to_datetime(end_date)]
            
        df = df.sort_values('date').set_index('date')
        
        # åŸºæœ¬æ•°æ®æ¸…æ´—
        df = df.dropna(subset=['close'])
        
        logger.info(f"æˆåŠŸèŽ·å–æ•°æ®: {len(df)}è¡Œ, æ—¥æœŸèŒƒå›´: {df.index.min().strftime('%Y-%m-%d')} åˆ° {df.index.max().strftime('%Y-%m-%d')}")
        return df[['close']].rename(columns={'close': 'price'})
    except Exception as e:
        logger.error(f"èŽ·å–æ•°æ®å¤±è´¥: {str(e)}")
        raise

# æ•°æ®é¢„å¤„ç†ä¸Žç‰¹å¾å·¥ç¨‹æ¨¡å—
def engineer_features(df):
    """å¯¹åŽŸå§‹æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹
    
    Args:
        df: åŽŸå§‹ä»·æ ¼æ•°æ®DataFrame
        
    Returns:
        æ·»åŠ ç‰¹å¾åŽçš„DataFrame
    """
    df_feat = df.copy()
    
    # ä¿ç•™åŸºæœ¬ç‰¹å¾
    df_feat['MA5'] = df_feat['price'].rolling(window=5).mean()
    df_feat['MA20'] = df_feat['price'].rolling(window=20).mean()
    df_feat['volatility_20'] = df_feat['price'].rolling(window=20).std()
    df_feat['price_change'] = df_feat['price'].pct_change()
    
    # ç§»é™¤NaNå€¼
    df_feat = df_feat.dropna()
    
    logger.info(f"å®Œæˆç‰¹å¾å·¥ç¨‹ï¼Œç‰¹å¾æ•°é‡: {df_feat.shape[1]}")
    return df_feat

# æ•°æ®é¢„å¤„ç†æ¨¡å—
def preprocess_data(data, window=60, future=5, test_size=0.2, feature_columns=None):
    """åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬"""
    if feature_columns is None:
        feature_columns = data.columns.tolist()
    
    # é€‰æ‹©ç‰¹å¾
    features = data[feature_columns]
    
    # åˆ›å»ºä¸“ç”¨äºŽä»·æ ¼çš„scaler
    price_scaler = MinMaxScaler(feature_range=(0, 1))
    price_data = data[['price']].values
    price_scaler.fit(price_data)
    
    # å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_features = scaler.fit_transform(features)
    
    # è®°å½•ç‰¹å¾æ•°é‡å’Œä»·æ ¼åˆ—ç´¢å¼•
    n_features = len(feature_columns)
    price_idx = feature_columns.index('price')
    
    # åˆ›å»ºåºåˆ—æ•°æ®
    X, y = [], []
    for i in range(window, len(scaled_features) - future + 1):
        # çª—å£åºåˆ— - æ‰€æœ‰ç‰¹å¾
        X.append(scaled_features[i-window:i])
        # æœªæ¥ä»·æ ¼ - åªå–ä»·æ ¼åˆ—
        y.append(scaled_features[i:i+future, price_idx])
    
    X = np.array(X)
    y = np.array(y)
    
    # è¾“å‡ºåºåˆ—çš„å½¢çŠ¶ï¼Œç”¨äºŽè°ƒè¯•
    logger.info(f"X.shape = {X.shape}, y.shape = {y.shape}")
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    split = int((1 - test_size) * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    
    return X_train, y_train, X_test, y_test, price_scaler

# æ¨¡åž‹æž„å»ºæ¨¡å—
def build_model(input_shape, future_days=5, learning_rate=0.001):
    """æž„å»ºå¢žå¼ºç‰ˆLSTMæ¨¡åž‹
    
    Args:
        input_shape: è¾“å…¥æ•°æ®å½¢çŠ¶ï¼Œå…ƒç»„(æ—¶é—´æ­¥æ•°, ç‰¹å¾æ•°)
        future_days: é¢„æµ‹æœªæ¥çš„å¤©æ•°
        learning_rate: å­¦ä¹ çŽ‡
        
    Returns:
        ç¼–è¯‘å¥½çš„Kerasæ¨¡åž‹
    """
    model = Sequential([
        # å‡å°‘å¤æ‚åº¦
        LSTM(64, return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        LSTM(32, return_sequences=False),
        Dense(future_days)
    ])
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mse')
    
    logger.info(f"æ¨¡åž‹æž„å»ºå®Œæˆ: {model.summary()}")
    return model

# è¯„ä¼°æ¨¡å—
def evaluate_model(model, X_test, y_test, scaler):
    """è¯„ä¼°æ¨¡åž‹æ€§èƒ½"""
    # é¢„æµ‹
    predictions = model.predict(X_test)
    
    # åå½’ä¸€åŒ– - ç®€åŒ–ç‰ˆæœ¬
    actual = scaler.inverse_transform(y_test.reshape(-1, 1))
    pred = scaler.inverse_transform(predictions.reshape(-1, 1))
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = mean_absolute_error(actual, pred)
    rmse = np.sqrt(mean_squared_error(actual, pred))
    r2 = r2_score(actual, pred)
    
    logger.info(f"æ¨¡åž‹è¯„ä¼°: MAE={mae:.2f}, RMSE={rmse:.2f}, RÂ²={r2:.4f}")
    
    return actual, pred, {'mae': mae, 'rmse': rmse, 'r2': r2}

# å¯è§†åŒ–æ¨¡å—
def plot_results(actual, pred, history=None, predictions_days=5, use_english=False):
    """å¯è§†åŒ–ç»“æžœ
    
    Args:
        actual: å®žé™…å€¼
        pred: é¢„æµ‹å€¼
        history: è®­ç»ƒåŽ†å²è®°å½•
        predictions_days: é¢„æµ‹çš„å¤©æ•°
        use_english: æ˜¯å¦ä½¿ç”¨è‹±æ–‡æ ‡é¢˜
    """
    plt.style.use('seaborn-v0_8-darkgrid')
    
    # åˆ›å»ºä¸€ä¸ª2x2çš„å­å›¾å¸ƒå±€
    fig, axs = plt.subplots(2, 2, figsize=(20, 12))
    
    # è®¾ç½®æ ‡é¢˜è¯­è¨€
    if use_english:
        title1 = f"Gold Price Prediction (Next {predictions_days} days, MAE: {mean_absolute_error(actual, pred):.2f})"
        title2 = "Predicted vs Actual Values"
        title3 = "Prediction Error Distribution"
        title4 = "Training and Validation Loss"
        xlabel1 = "Samples"
        ylabel1 = "Price (CNY/g)"
        xlabel2 = "Actual Price"
        ylabel2 = "Predicted Price"
        xlabel3 = "Prediction Error"
        ylabel3 = "Frequency"
        xlabel4 = "Epoch"
        ylabel4 = "Loss"
        legend1 = ["Actual Price", "Predicted Price"]
        legend2 = ["Training Loss", "Validation Loss"]
    else:
        title1 = f"é»„é‡‘ä»·æ ¼é¢„æµ‹ (æœªæ¥{predictions_days}å¤© MAE: {mean_absolute_error(actual, pred):.2f}å…ƒ)"
        title2 = "é¢„æµ‹å€¼ vs å®žé™…å€¼"
        title3 = "é¢„æµ‹è¯¯å·®åˆ†å¸ƒ"
        title4 = "è®­ç»ƒå’ŒéªŒè¯æŸå¤±"
        xlabel1 = "æ ·æœ¬"
        ylabel1 = "ä»·æ ¼ (å…ƒ/å…‹)"
        xlabel2 = "å®žé™…ä»·æ ¼"
        ylabel2 = "é¢„æµ‹ä»·æ ¼"
        xlabel3 = "é¢„æµ‹è¯¯å·®"
        xlabel4 = "Epoch"
        ylabel4 = "æŸå¤±"
        legend1 = ["å®žé™…ä»·æ ¼", "é¢„æµ‹ä»·æ ¼"]
        legend2 = ["è®­ç»ƒæŸå¤±", "éªŒè¯æŸå¤±"]
    
    # 1. ä»·æ ¼é¢„æµ‹å¯¹æ¯”å›¾
    axs[0, 0].plot(actual, label=legend1[0], alpha=0.7, linewidth=2)
    axs[0, 0].plot(pred, label=legend1[1], linestyle='--', linewidth=2)
    axs[0, 0].set_title(title1, fontsize=15)
    axs[0, 0].set_xlabel(xlabel1, fontsize=12)
    axs[0, 0].set_ylabel(ylabel1, fontsize=12)
    axs[0, 0].legend(fontsize=12)
    axs[0, 0].grid(True)
    
    # 2. ä»·æ ¼é¢„æµ‹æ•£ç‚¹å›¾
    axs[0, 1].scatter(actual, pred, alpha=0.5)
    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())
    axs[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--')
    axs[0, 1].set_title(title2, fontsize=15)
    axs[0, 1].set_xlabel(xlabel2, fontsize=12)
    axs[0, 1].set_ylabel(ylabel2, fontsize=12)
    
    # 3. ä»·æ ¼è¯¯å·®ç›´æ–¹å›¾
    error = actual - pred
    sns.histplot(error, bins=50, kde=True, ax=axs[1, 0])
    axs[1, 0].set_title(title3, fontsize=15)
    axs[1, 0].set_xlabel(xlabel3, fontsize=12)
    axs[1, 0].set_ylabel(ylabel3, fontsize=12)
    
    # 4. è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿ (å¦‚æžœæä¾›äº†history)
    if history:
        axs[1, 1].plot(history.history['loss'], label=legend2[0])
        axs[1, 1].plot(history.history['val_loss'], label=legend2[1])
        axs[1, 1].set_title(title4, fontsize=15)
        axs[1, 1].set_xlabel(xlabel4, fontsize=12)
        axs[1, 1].set_ylabel(ylabel4, fontsize=12)
        axs[1, 1].legend(fontsize=12)
    else:
        axs[1, 1].set_visible(False)
    
    plt.tight_layout()
    language = "en" if use_english else "cn"
    plt.savefig(f'gold_price_prediction_results_{language}.png', dpi=300, bbox_inches='tight')
    plt.show()

# ä¿å­˜æ¨¡åž‹å…ƒæ•°æ®å‡½æ•°
def save_model_metadata(model_path: str, metrics: Dict[str, Any], data_info: Dict[str, Any]) -> str:
    """ä¿å­˜æ¨¡åž‹å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬è®­ç»ƒæ—¥æœŸå’Œæ€§èƒ½æŒ‡æ ‡
    
    Args:
        model_path: æ¨¡åž‹æ–‡ä»¶è·¯å¾„
        metrics: æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡
        data_info: è®­ç»ƒæ•°æ®ä¿¡æ¯
        
    Returns:
        å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    """
    metadata = {
        "model_path": model_path,
        "training_date": datetime.datetime.now().strftime("%Y-%m-%d"),
        "metrics": metrics,
        "data_info": data_info,
        "next_training_date": (datetime.datetime.now() + datetime.timedelta(days=90)).strftime("%Y-%m-%d")
    }
    
    # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    metadata_path = model_path.replace('.keras', '_metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=4)
    
    # åŒæ—¶æ›´æ–°"æœ€æ–°æ¨¡åž‹"çš„å…ƒæ•°æ®é“¾æŽ¥
    latest_metadata_path = os.path.join(os.path.dirname(model_path), 'latest_model_metadata.json')
    with open(latest_metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=4)
    
    logger.info(f"æ¨¡åž‹å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_path}")
    return metadata_path

# æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒæ¨¡åž‹
def should_retrain_model(retraining_period_days: int = 90) -> Tuple[bool, str]:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦æ ¹æ®æ—¶é—´é—´éš”é‡æ–°è®­ç»ƒæ¨¡åž‹
    
    Args:
        retraining_period_days: é‡æ–°è®­ç»ƒå‘¨æœŸï¼ˆå¤©ï¼‰
        
    Returns:
        (æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒ, åŽŸå› è¯´æ˜Ž)
    """
    model_dir = Path('models')
    latest_metadata_path = model_dir / 'latest_model_metadata.json'
    
    # å¦‚æžœæ²¡æœ‰å…ƒæ•°æ®æ–‡ä»¶ï¼Œåˆ™éœ€è¦è®­ç»ƒæ–°æ¨¡åž‹
    if not latest_metadata_path.exists():
        return True, "æ²¡æœ‰æ‰¾åˆ°çŽ°æœ‰æ¨¡åž‹å…ƒæ•°æ®ï¼Œéœ€è¦è®­ç»ƒæ–°æ¨¡åž‹"
    
    try:
        # è¯»å–å…ƒæ•°æ®
        with open(latest_metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # èŽ·å–æœ€åŽè®­ç»ƒæ—¥æœŸå’Œè®¡åˆ’ä¸‹æ¬¡è®­ç»ƒæ—¥æœŸ
        last_training_date = datetime.datetime.strptime(metadata['training_date'], "%Y-%m-%d")
        next_training_date = metadata.get('next_training_date')
        
        if next_training_date:
            next_training_date = datetime.datetime.strptime(next_training_date, "%Y-%m-%d")
        else:
            # å¦‚æžœæ²¡æœ‰ä¸‹æ¬¡è®­ç»ƒæ—¥æœŸï¼Œåˆ™åŸºäºŽæœ€åŽè®­ç»ƒæ—¥æœŸå’Œé‡æ–°è®­ç»ƒå‘¨æœŸè®¡ç®—
            next_training_date = last_training_date + datetime.timedelta(days=retraining_period_days)
        
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾é‡æ–°è®­ç»ƒæ—¶é—´
        current_date = datetime.datetime.now()
        if current_date >= next_training_date:
            days_since_last_training = (current_date - last_training_date).days
            return True, f"è·ç¦»ä¸Šæ¬¡è®­ç»ƒå·²ç»è¿‡åŽ» {days_since_last_training} å¤©ï¼Œè¶…è¿‡äº†è®¾å®šçš„ {retraining_period_days} å¤©é‡æ–°è®­ç»ƒå‘¨æœŸ"
        else:
            days_until_next_training = (next_training_date - current_date).days
            return False, f"è·ç¦»ä¸‹æ¬¡è®¡åˆ’è®­ç»ƒè¿˜æœ‰ {days_until_next_training} å¤©ï¼Œå½“å‰æ¨¡åž‹ä¾ç„¶æœ‰æ•ˆ"
    
    except Exception as e:
        logger.warning(f"æ£€æŸ¥æ¨¡åž‹è®­ç»ƒçŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
        return True, f"æ£€æŸ¥æ¨¡åž‹çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ: {str(e)}"

# èŽ·å–æœ€æ–°æ¨¡åž‹è·¯å¾„
def get_latest_model_path() -> Optional[str]:
    """èŽ·å–æœ€æ–°è®­ç»ƒçš„æ¨¡åž‹è·¯å¾„
    
    Returns:
        æœ€æ–°æ¨¡åž‹è·¯å¾„ï¼Œå¦‚æžœæ²¡æœ‰åˆ™è¿”å›žNone
    """
    model_dir = Path('models')
    latest_metadata_path = model_dir / 'latest_model_metadata.json'
    
    if not latest_metadata_path.exists():
        return None
    
    try:
        with open(latest_metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        model_path = metadata['model_path']
        if os.path.exists(model_path):
            return model_path
    except Exception as e:
        logger.warning(f"èŽ·å–æœ€æ–°æ¨¡åž‹è·¯å¾„æ—¶å‡ºé”™: {str(e)}")
    
    return None

# ä¸»ç¨‹åº
def main(window=60, future_days=5, epochs=100, batch_size=32, force_retrain=False, retraining_period_days=90):
    """ä¸»å‡½æ•°
    
    Args:
        window: æ—¶é—´çª—å£å¤§å°
        future_days: é¢„æµ‹æœªæ¥çš„å¤©æ•°
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹é‡å¤§å°
        force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡åž‹
        retraining_period_days: æ¨¡åž‹é‡æ–°è®­ç»ƒå‘¨æœŸï¼ˆå¤©ï¼‰
    """
    try:
        # åˆ›å»ºæ¨¡åž‹ä¿å­˜ç›®å½•
        model_dir = 'models'
        os.makedirs(model_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒæ¨¡åž‹
        should_train, reason = should_retrain_model(retraining_period_days)
        
        # å¦‚æžœå¼ºåˆ¶é‡æ–°è®­ç»ƒæˆ–è€…åº”è¯¥é‡æ–°è®­ç»ƒ
        if force_retrain or should_train:
            logger.info(f"å°†é‡æ–°è®­ç»ƒæ¨¡åž‹: {reason}")
            
            # èŽ·å–å½“å‰æ—¶é—´ï¼Œç”¨äºŽæ¨¡åž‹æ–‡ä»¶å‘½å
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            model_path = os.path.join(model_dir, f'gold_prediction_model_{timestamp}.keras')
            
            # èŽ·å–æ•°æ®
            df = fetch_gold_data()
            print(f"æœ€æ–°æ•°æ®æ—¥æœŸï¼š{df.index[-1].strftime('%Y-%m-%d')}")
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            print(f"æ•°æ®ç»Ÿè®¡: \n{df.describe()}")
            
            # ç»˜åˆ¶åŽŸå§‹æ•°æ®å›¾è¡¨ï¼Œæ£€æŸ¥å¼‚å¸¸å€¼å’Œè¶‹åŠ¿
            plt.figure(figsize=(12,6))
            plt.plot(df.index, df['price'])
            plt.title('é»„é‡‘ä»·æ ¼èµ°åŠ¿')
            plt.grid(True)
            plt.savefig('gold_price_trend.png')
            plt.close()
            
            # ç‰¹å¾ç›¸å…³æ€§åˆ†æž
            df_features = engineer_features(df)
            corr = df_features.corr()
            plt.figure(figsize=(14,10))
            sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
            plt.savefig('feature_correlation.png')
            plt.close()
            
            # æ•°æ®é¢„å¤„ç†
            X_train, y_train, X_test, y_test, scaler = preprocess_data(
                df_features, 
                window=window, 
                future=future_days,
                test_size=0.2
            )
            
            # å­¦ä¹ çŽ‡ä½™å¼¦é€€ç«
            lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
                initial_learning_rate=0.001,
                decay_steps=epochs * (len(X_train) // batch_size),
                alpha=0.0001
            )
            optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
            
            # æž„å»ºæ¨¡åž‹
            model = build_model((X_train.shape[1], X_train.shape[2]), future_days)
            
            # åˆ›å»ºå›žè°ƒå‡½æ•°
            callbacks = [
                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)
            ]
            
            # æ¨¡åž‹è®­ç»ƒ
            logger.info("å¼€å§‹è®­ç»ƒæ¨¡åž‹...")
            history = model.fit(
                X_train, y_train,
                epochs=50,  # å‡å°‘åˆå§‹epochs
                batch_size=16,  # å‡å°‘æ‰¹é‡å¤§å°
                validation_split=0.2,
                callbacks=callbacks,
                verbose=1
            )
            
            # è¯„ä¼°æ¨¡åž‹
            logger.info("è¯„ä¼°æ¨¡åž‹æ€§èƒ½...")
            actual, pred, metrics = evaluate_model(model, X_test, y_test, scaler)
            
            # æ‰“å°è¯„ä¼°æŒ‡æ ‡
            print(f"æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡:")
            print(f"MAE: {metrics['mae']:.2f} å…ƒ/å…‹")
            print(f"RMSE: {metrics['rmse']:.2f} å…ƒ/å…‹")
            print(f"RÂ²: {metrics['r2']:.2%}")
            
            # å¯è§†åŒ–ç»“æžœ - æŠ€æœ¯æ€§å›¾è¡¨
            logger.info("ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨...")
            plot_results(actual, pred, history, future_days, use_english=True)
            
            # æ·»åŠ æ™®é€šç”¨æˆ·å‹å¥½çš„è¶‹åŠ¿å›¾è¡¨
            logger.info("ç”Ÿæˆé¢å‘æ™®é€šç”¨æˆ·çš„æœªæ¥è¶‹åŠ¿å›¾...")
            plot_future_trend(future_days=future_days, use_english=True)
            
            # ä¿å­˜æ¨¡åž‹ä½¿ç”¨æ–°æ ¼å¼
            model.save(model_path)
            logger.info(f"æ¨¡åž‹å·²ä¿å­˜åˆ°: {model_path}")
            
            # åŒæ—¶ä¿å­˜ä¸€ä¸ªlatestæ¨¡åž‹å‰¯æœ¬ï¼Œæ–¹ä¾¿åŽç»­åŠ è½½
            latest_model_path = os.path.join(model_dir, 'gold_prediction_model_latest.keras')
            model.save(latest_model_path)
            
            # ä¿å­˜æ¨¡åž‹å…ƒæ•°æ®
            data_info = {
                "data_range": {
                    "start": df.index.min().strftime('%Y-%m-%d'),
                    "end": df.index.max().strftime('%Y-%m-%d')
                },
                "data_count": len(df),
                "feature_count": df_features.shape[1]
            }
            save_model_metadata(model_path, metrics, data_info)
            
            return model, scaler, metrics
        
        else:
            # åŠ è½½çŽ°æœ‰æ¨¡åž‹è¿›è¡Œé¢„æµ‹
            logger.info(f"ä½¿ç”¨çŽ°æœ‰æ¨¡åž‹: {reason}")
            model_path = get_latest_model_path()
            
            if not model_path:
                logger.warning("æ— æ³•æ‰¾åˆ°çŽ°æœ‰æ¨¡åž‹ï¼Œå°†é‡æ–°è®­ç»ƒ")
                return main(window, future_days, epochs, batch_size, force_retrain=True)
            
            # åŠ è½½æ¨¡åž‹å’Œå…ƒæ•°æ®
            model = load_model(model_path)
            
            with open(Path('models') / 'latest_model_metadata.json', 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # èŽ·å–æœ€æ–°çš„ä¸€äº›æ•°æ®ç”¨äºŽæµ‹è¯•å’Œé¢„æµ‹
            df = fetch_gold_data()
            df_features = engineer_features(df)
            
            logger.info(f"åŠ è½½çŽ°æœ‰æ¨¡åž‹æˆåŠŸï¼Œä¸Šæ¬¡è®­ç»ƒæ—¥æœŸ: {metadata['training_date']}")
            print(f"çŽ°æœ‰æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡:")
            print(f"MAE: {metadata['metrics']['mae']:.2f} å…ƒ/å…‹")
            print(f"RMSE: {metadata['metrics']['rmse']:.2f} å…ƒ/å…‹")
            print(f"RÂ²: {metadata['metrics']['r2']:.2%}")
            
            # ç”Ÿæˆé¢å‘æ™®é€šç”¨æˆ·çš„æœªæ¥è¶‹åŠ¿å›¾
            logger.info("ç”Ÿæˆé¢å‘æ™®é€šç”¨æˆ·çš„æœªæ¥è¶‹åŠ¿å›¾...")
            plot_future_trend(future_days=future_days, use_english=True)
            
            # å¦‚æžœéœ€è¦ï¼Œå¯ä»¥æ·»åŠ å¯¹æ¨¡åž‹è¿›è¡Œç®€å•è¯„ä¼°çš„ä»£ç 
            
            return model, None, metadata['metrics']
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        raise

def build_transformer_model(input_shape, future_days=5, head_size=256, num_heads=4):
    """æž„å»ºåŸºäºŽTransformerçš„é¢„æµ‹æ¨¡åž‹"""
    inputs = tf.keras.Input(shape=input_shape)
    
    # ä½ç½®ç¼–ç 
    x = inputs
    
    # Transformerç¼–ç å™¨å±‚
    x = tf.keras.layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads)(x, x)
    x = tf.keras.layers.LayerNormalization()(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    
    # å‰é¦ˆç½‘ç»œ
    x = tf.keras.layers.Conv1D(filters=head_size, kernel_size=1, activation='relu')(x)
    x = tf.keras.layers.LayerNormalization()(x)
    
    # å…¨å±€æ± åŒ–
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = tf.keras.layers.Dense(64, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    outputs = tf.keras.layers.Dense(future_days)(x)
    
    model = tf.keras.Model(inputs, outputs)
    return model

def k_fold_validation(X, y, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_metrics = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        model = build_model((X_train.shape[1], X_train.shape[2]))
        # è®­ç»ƒæ¨¡åž‹
        history = model.fit(X_train, y_train, 
                          epochs=50, batch_size=32, 
                          validation_data=(X_val, y_val),
                          callbacks=[EarlyStopping(patience=10)],
                          verbose=0)
        # è¯„ä¼°æ¨¡åž‹
        metrics = evaluate_model(model, X_val, y_val, scaler)
        fold_metrics.append(metrics)
    
    return fold_metrics

def evaluate_forecast(actual, pred):
    # å·²æœ‰è¯„ä¼°æŒ‡æ ‡
    mae = mean_absolute_error(actual, pred)
    mse = mean_squared_error(actual, pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(actual, pred)
    
    # æ·»åŠ æ–°æŒ‡æ ‡
    mape = np.mean(np.abs((actual - pred) / actual)) * 100  # å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®
    
    # æ–¹å‘å‡†ç¡®æ€§ - é¢„æµ‹ä»·æ ¼å˜åŠ¨æ–¹å‘æ˜¯å¦æ­£ç¡®
    actual_dir = np.sign(np.diff(actual.reshape(-1)))
    pred_dir = np.sign(np.diff(pred.reshape(-1)))
    dir_accuracy = np.mean(actual_dir == pred_dir)
    
    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'mape': mape,
        'direction_accuracy': dir_accuracy
    }

def analyze_feature_importance(model, X, feature_names):
    # æŽ’åˆ—é‡è¦æ€§
    perm_importance = []
    baseline_pred = model.predict(X)
    baseline_mae = mean_absolute_error(scaler.inverse_transform(y_test), 
                                       scaler.inverse_transform(baseline_pred))
    
    for i, feature in enumerate(feature_names):
        # æ‰“ä¹±ç‰¹å¾å€¼
        X_permuted = X.copy()
        X_permuted[:, :, i] = np.random.permutation(X_permuted[:, :, i])
        
        # é¢„æµ‹å¹¶è®¡ç®—è¯¯å·®
        perm_pred = model.predict(X_permuted)
        perm_mae = mean_absolute_error(scaler.inverse_transform(y_test), 
                                       scaler.inverse_transform(perm_pred))
        
        # é‡è¦æ€§ = æ‰“ä¹±åŽè¯¯å·®å¢žåŠ é‡
        importance = perm_mae - baseline_mae
        perm_importance.append((feature, importance))
    
    # æŒ‰é‡è¦æ€§æŽ’åº
    perm_importance.sort(key=lambda x: x[1], reverse=True)
    return perm_importance

def generate_predictions(days=30):
    """ç”Ÿæˆæœªæ¥30å¤©çš„é¢„æµ‹"""
    model = load_model('models/gold_prediction_model_latest.keras')  # æ›´æ–°æ–‡ä»¶æ‰©å±•åä¸º.keras
    
    # èŽ·å–æœ€æ–°æ•°æ®
    df = fetch_gold_data()
    df_features = engineer_features(df)
    
    # å‡†å¤‡é¢„æµ‹è¾“å…¥ - æœ€æ–°çš„çª—å£
    window = 60  # æ·»åŠ é»˜è®¤çª—å£å¤§å°
    latest_data = df_features.iloc[-window:].values
    scaler = MinMaxScaler()
    scaler.fit(df_features[['price']].values)  # åªå¯¹ä»·æ ¼åˆ—è¿›è¡Œæ‹Ÿåˆ
    
    # å¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
    features_scaler = MinMaxScaler()
    latest_scaled = features_scaler.fit_transform(latest_data)
    X_pred = latest_scaled.reshape(1, window, df_features.shape[1])
    
    # é€’å½’é¢„æµ‹
    predictions = []
    current_sequence = X_pred.copy()
    
    for _ in range(days):
        # é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
        next_pred = model.predict(current_sequence, verbose=0)[0]
        predictions.append(next_pred[0])
        
        # æ›´æ–°åºåˆ—
        next_input = np.zeros((1, 1, df_features.shape[1]))
        next_input[0, 0, 0] = next_pred[0]  # å‡è®¾priceæ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾
        
        # æ»šåŠ¨çª—å£
        current_sequence = np.append(current_sequence[:, 1:, :], 
                                     next_input, axis=1)
    
    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æžœ
    predictions = scaler.inverse_transform(
        np.array(predictions).reshape(-1, 1))
    
    # åˆ›å»ºæ—¥æœŸç´¢å¼• - ä»Žæœ€æ–°æ•°æ®çš„ä¸‹ä¸€å¤©å¼€å§‹
    last_date = df.index[-1]
    future_dates = [last_date + datetime.timedelta(days=i+1) for i in range(days)]
    
    return predictions, future_dates

# é’ˆå¯¹æ™®é€šç”¨æˆ·çš„ç›´è§‚å¯è§†åŒ–å‡½æ•°
def plot_future_trend(future_days=5, use_english=False, use_sample_data=False):
    """åˆ›å»ºæ›´ç›´è§‚çš„æœªæ¥é‡‘ä»·èµ°åŠ¿å›¾
    
    Args:
        future_days: é¢„æµ‹çš„æœªæ¥å¤©æ•°
        use_english: æ˜¯å¦ä½¿ç”¨è‹±æ–‡
        use_sample_data: å½“æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡åž‹æ—¶ï¼Œæ˜¯å¦ä½¿ç”¨æ ·ä¾‹æ•°æ®
    """
    try:
        # èŽ·å–é¢„æµ‹æ•°æ®
        predictions, future_dates = generate_predictions(future_days)
        predictions = predictions.flatten()
    except Exception as e:
        logger.warning(f"èŽ·å–é¢„æµ‹æ•°æ®å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨æ ·ä¾‹æ•°æ®")
        if not use_sample_data:
            raise
            
        # ä½¿ç”¨æ ·ä¾‹æ•°æ®åˆ›å»ºç¤ºä¾‹å›¾è¡¨
        df = fetch_gold_data()
        last_date = df.index[-1]
        future_dates = [last_date + datetime.timedelta(days=i+1) for i in range(future_days)]
        
        # ç”Ÿæˆä¸€äº›æ¨¡æ‹Ÿçš„é¢„æµ‹ç»“æžœ - ä»¥æœ€åŽä¸€ä¸ªä»·æ ¼ä¸ºåŸºå‡†ï¼Œæ·»åŠ ä¸€äº›éšæœºæ³¢åŠ¨
        last_price = df['price'].iloc[-1]
        # ä½¿ç”¨éšæœºç§å­ç¡®ä¿æ¯æ¬¡ç”Ÿæˆç›¸åŒç»“æžœ
        np.random.seed(42)
        # ç”Ÿæˆä¸€ä¸ªè¶‹åŠ¿å¢žé•¿æˆ–ä¸‹é™çš„åºåˆ—ï¼Œå†åŠ ä¸Šéšæœºæ³¢åŠ¨
        trend = np.linspace(0, 0.05, future_days)  # ç”Ÿæˆ0åˆ°5%çš„çº¿æ€§å¢žé•¿
        noise = np.random.normal(0, 0.01, future_days)  # æ·»åŠ 1%å·¦å³çš„éšæœºæ³¢åŠ¨
        predictions = last_price * (1 + trend + noise)
        
        logger.warning("ä½¿ç”¨æ ·ä¾‹æ•°æ®åˆ›å»ºçš„ç¤ºä¾‹å›¾è¡¨ï¼Œä»…ä¾›å‚è€ƒ")
    
    # èŽ·å–åŽ†å²æ•°æ®ä½œä¸ºå‚è€ƒ
    df = fetch_gold_data()
    recent_days = 30  # æ˜¾ç¤ºæœ€è¿‘30å¤©çš„åŽ†å²æ•°æ®
    historical = df['price'].iloc[-recent_days:].copy()
    
    # è®¾ç½®å­—ä½“å’Œæ ·å¼
    plt.style.use('seaborn-v0_8')
    plt.figure(figsize=(14, 8))
    
    # è®¾ç½®è¯­è¨€
    if use_english:
        title = "Gold Price Forecast (Next 5 Days)"
        subtitle = f"Last updated: {datetime.datetime.now().strftime('%Y-%m-%d')}"
        ylabel = "Price (CNY/gram)"
        trend_label = "Price Trend"
        history_label = "Historical Price"
        prediction_label = "Predicted Price"
        today_label = "Today"
        note_text = "Note: This forecast is based on historical data patterns and may vary due to market conditions."
        if predictions[-1] > predictions[0]:
            trend_text = f"ðŸ”¼ Upward Trend: Price expected to increase by {predictions[-1]-predictions[0]:.2f} CNY/gram ({(predictions[-1]/predictions[0]-1)*100:.1f}%)"
            trend_color = 'green'
        elif predictions[-1] < predictions[0]:
            trend_text = f"ðŸ”½ Downward Trend: Price expected to decrease by {predictions[0]-predictions[-1]:.2f} CNY/gram ({(1-predictions[-1]/predictions[0])*100:.1f}%)"
            trend_color = 'red'
        else:
            trend_text = "â—€ï¸â–¶ï¸ Stable Price: No significant change expected"
            trend_color = 'blue'
        
        # æ·»åŠ æ ·ä¾‹æ•°æ®è­¦å‘Š
        if use_sample_data:
            sample_warning = "âš ï¸ SAMPLE DATA - FOR DEMONSTRATION ONLY âš ï¸"
    else:
        title = "é»„é‡‘ä»·æ ¼é¢„æµ‹ï¼ˆæœªæ¥5å¤©ï¼‰"
        subtitle = f"æœ€åŽæ›´æ–°ï¼š{datetime.datetime.now().strftime('%Y-%m-%d')}"
        ylabel = "ä»·æ ¼ï¼ˆå…ƒ/å…‹ï¼‰"
        trend_label = "ä»·æ ¼è¶‹åŠ¿"
        history_label = "åŽ†å²ä»·æ ¼"
        prediction_label = "é¢„æµ‹ä»·æ ¼"
        today_label = "ä»Šå¤©"
        note_text = "æ³¨æ„ï¼šè¯¥é¢„æµ‹åŸºäºŽåŽ†å²æ•°æ®æ¨¡å¼ï¼Œå¯èƒ½å› å¸‚åœºæ¡ä»¶è€Œå˜åŒ–ã€‚"
        if predictions[-1] > predictions[0]:
            trend_text = f"ðŸ”¼ ä¸Šæ¶¨è¶‹åŠ¿ï¼šé¢„è®¡ä»·æ ¼å°†ä¸Šæ¶¨ {predictions[-1]-predictions[0]:.2f} å…ƒ/å…‹ ({(predictions[-1]/predictions[0]-1)*100:.1f}%)"
            trend_color = 'red'  # åœ¨ä¸­å›½æ–‡åŒ–ä¸­ï¼Œçº¢è‰²é€šå¸¸ä»£è¡¨ä¸Šæ¶¨
        elif predictions[-1] < predictions[0]:
            trend_text = f"ðŸ”½ ä¸‹è·Œè¶‹åŠ¿ï¼šé¢„è®¡ä»·æ ¼å°†ä¸‹è·Œ {predictions[0]-predictions[-1]:.2f} å…ƒ/å…‹ ({(1-predictions[-1]/predictions[0])*100:.1f}%)"
            trend_color = 'green'  # åœ¨ä¸­å›½æ–‡åŒ–ä¸­ï¼Œç»¿è‰²é€šå¸¸ä»£è¡¨ä¸‹è·Œ
        else:
            trend_text = "â—€ï¸â–¶ï¸ ä»·æ ¼ç¨³å®šï¼šé¢„è®¡æ— æ˜Žæ˜¾å˜åŒ–"
            trend_color = 'blue'
            
        # æ·»åŠ æ ·ä¾‹æ•°æ®è­¦å‘Š
        if use_sample_data:
            sample_warning = "âš ï¸ ç¤ºä¾‹æ•°æ® - ä»…ç”¨äºŽæ¼”ç¤º âš ï¸"
    
    # ç»˜åˆ¶åŽ†å²æ•°æ®
    plt.plot(historical.index, historical.values, 
             color='gray', alpha=0.7, linewidth=2, label=history_label)
    
    # åœ¨å›¾è¡¨ä¸Šæ ‡è®°"ä»Šå¤©"
    plt.axvline(x=df.index[-1], color='black', linestyle='--', alpha=0.7)
    plt.text(df.index[-1], historical.min() * 0.98, today_label, 
             ha='center', va='top', rotation=90, fontsize=10)
    
    # ç»˜åˆ¶é¢„æµ‹æ•°æ®
    prediction_line = plt.plot(future_dates, predictions, 
                              color=trend_color, marker='o', markersize=8, 
                              linewidth=3, label=prediction_label)[0]
    
    # å¡«å……é¢„æµ‹åŒºåŸŸï¼Œå¢žå¼ºè§†è§‰æ•ˆæžœ
    plt.fill_between(future_dates, predictions, 
                     df['price'].iloc[-1], alpha=0.2, color=trend_color)
    
    # ä¸ºæ¯ä¸ªé¢„æµ‹ç‚¹æ·»åŠ ä»·æ ¼æ ‡ç­¾
    for i, (date, price) in enumerate(zip(future_dates, predictions)):
        plt.annotate(f'{price:.1f}', (date, price), 
                     textcoords="offset points", 
                     xytext=(0,10), ha='center',
                     fontweight='bold', fontsize=12)
    
    # æ·»åŠ è¶‹åŠ¿æŒ‡ç¤ºæ–‡æœ¬æ¡†
    plt.figtext(0.5, 0.01, trend_text, 
               ha='center', fontsize=14, fontweight='bold',
               bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))
    
    # æ·»åŠ æ ·ä¾‹æ•°æ®è­¦å‘Š
    if use_sample_data:
        plt.figtext(0.5, 0.95, sample_warning, 
                   ha='center', fontsize=16, fontweight='bold', color='red',
                   bbox=dict(facecolor='yellow', alpha=0.8, boxstyle='round,pad=0.5'))
    
    # æ·»åŠ æ³¨é‡Šè¯´æ˜Ž
    plt.figtext(0.5, -0.02, note_text, ha='center', fontsize=10, style='italic')
    
    # è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾
    plt.title(title, fontsize=18, fontweight='bold')
    plt.suptitle(subtitle, fontsize=10, y=0.92)
    plt.ylabel(ylabel, fontsize=12)
    plt.grid(True, alpha=0.3)
    
    # æ ¼å¼åŒ–xè½´æ—¥æœŸ
    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%m-%d'))
    plt.xticks(rotation=45)
    
    # è°ƒæ•´yè½´èŒƒå›´ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„ç©ºé—´æ˜¾ç¤ºæ³¨é‡Š
    y_min = min(historical.min(), predictions.min()) * 0.98
    y_max = max(historical.max(), predictions.max()) * 1.02
    plt.ylim(y_min, y_max)
    
    # æ·»åŠ å›¾ä¾‹
    plt.legend(loc='upper left')
    
    # ç¡®ä¿å¸ƒå±€æ­£ç¡®
    plt.tight_layout(rect=[0, 0.04, 1, 0.96])
    
    # ä¿å­˜å›¾è¡¨
    language = "en" if use_english else "cn"
    plt.savefig(f'gold_future_trend_{language}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # è¿”å›žé¢„æµ‹ç»“æžœå’Œæ—¥æœŸï¼Œä»¥ä¾¿å¯èƒ½çš„è¿›ä¸€æ­¥ä½¿ç”¨
    return predictions, future_dates

# æ•°æ®åŠ è½½æ—¶è¿›è¡Œå†…å­˜ä¼˜åŒ–
def optimize_dataframe(df):
    """é™ä½ŽDataFrameå†…å­˜ä½¿ç”¨"""
    for col in df.select_dtypes(include=['float']):
        df[col] = pd.to_numeric(df[col], downcast='float')
    return df

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='é»„é‡‘ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ')
    parser.add_argument('--force-retrain', action='store_true', help='å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡åž‹')
    parser.add_argument('--retrain-period', type=int, default=90, help='æ¨¡åž‹é‡æ–°è®­ç»ƒå‘¨æœŸï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤90å¤©')
    parser.add_argument('--window', type=int, default=60, help='åŽ†å²çª—å£å¤§å°')
    parser.add_argument('--future', type=int, default=5, help='é¢„æµ‹æœªæ¥å¤©æ•°')
    parser.add_argument('--user-friendly', action='store_true', help='åªç”Ÿæˆé¢å‘æ™®é€šç”¨æˆ·çš„è¶‹åŠ¿å›¾')
    parser.add_argument('--chinese', action='store_true', help='ä½¿ç”¨ä¸­æ–‡ç”Ÿæˆå›¾è¡¨')
    parser.add_argument('--sample', action='store_true', help='ä½¿ç”¨æ ·ä¾‹æ•°æ®ç”Ÿæˆè¶‹åŠ¿å›¾ï¼ˆå½“æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡åž‹æ—¶ï¼‰')
    
    args = parser.parse_args()
    
    # å¦‚æžœåªéœ€è¦ç”Ÿæˆç”¨æˆ·å‹å¥½çš„è¶‹åŠ¿å›¾ï¼Œåˆ™ä¸è¿›è¡Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹
    if args.user_friendly:
        use_english = not args.chinese
        plot_future_trend(future_days=args.future, use_english=use_english, use_sample_data=args.sample)
    else:
        main(
            window=args.window,
            future_days=args.future,
            epochs=100,
            batch_size=32,
            force_retrain=args.force_retrain,
            retraining_period_days=args.retrain_period
        )